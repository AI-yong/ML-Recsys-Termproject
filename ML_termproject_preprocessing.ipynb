{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU7SWKFq9Mo7"
      },
      "source": [
        "# Check info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77z9Ido9VE7q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# goodreads_books info (first 100k samples)\n",
        "chunk_iterator = pd.read_json(\n",
        "    '/content/drive/MyDrive/data/goodreads_books.json',\n",
        "    lines=True,         # Format with one JSON object per line\n",
        "    encoding='latin-1', # Prevent encoding issues\n",
        "    chunksize=200_000   # Read in chunks of 100k\n",
        ")\n",
        "\n",
        "df_goodreads_books = next(chunk_iterator)\n",
        "print(df_goodreads_books.head())\n",
        "print(df_goodreads_books.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yhc6KGQmZoRc"
      },
      "outputs": [],
      "source": [
        "# goodreads_book_authors info (first 100k samples)\n",
        "chunk_iterator = pd.read_json(\n",
        "    '/content/drive/MyDrive/data/goodreads_book_authors.json',\n",
        "    lines=True,         # Format with one JSON object per line\n",
        "    encoding='latin-1', # Prevent encoding issues\n",
        "    chunksize=200_000   # Read in chunks of 100k\n",
        ")\n",
        "\n",
        "df_goodreads_book_authors = next(chunk_iterator)\n",
        "print(df_goodreads_book_authors.head())\n",
        "print(df_goodreads_book_authors.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWr6_JqEhQNo"
      },
      "outputs": [],
      "source": [
        "# goodreads_reviews_dedup info (first 100k samples)\n",
        "chunk_iterator = pd.read_json(\n",
        "    '/content/drive/MyDrive/data/goodreads_reviews_dedup.json',\n",
        "    lines=True,         # Format with one JSON object per line\n",
        "    encoding='latin-1', # Prevent encoding issues\n",
        "    chunksize=200_000   # Read in chunks of 100k\n",
        ")\n",
        "\n",
        "df_goodreads_reviews = next(chunk_iterator)\n",
        "print(df_goodreads_reviews.head())\n",
        "print(df_goodreads_reviews.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L5hmP6lhOgx"
      },
      "outputs": [],
      "source": [
        "# Books_5.json info (first 100k samples)\n",
        "\n",
        "\n",
        "chunk_iterator = pd.read_json(\n",
        "    '/content/drive/MyDrive/data/Books_5.json',\n",
        "    lines=True,           # Format with one JSON object per line\n",
        "    encoding='latin-1',   # Prevent encoding issues\n",
        "    chunksize=300_000     # Read in chunks of 100k\n",
        ")\n",
        "\n",
        "df_books_5 = next(chunk_iterator)\n",
        "print(df_books_5.head())\n",
        "print(df_books_5.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5XyGmwppV8A"
      },
      "outputs": [],
      "source": [
        "# meta_Books.json info (first 200k samples)\n",
        "import ast  # Needed to convert text with single quotes ('') into a dictionary\n",
        "\n",
        "data_list = []\n",
        "\n",
        "# Instead of pd.read_json, read the file line by line manually\n",
        "with open('/content/drive/MyDrive/data/meta_Books.json', 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= 200_000:\n",
        "            break\n",
        "\n",
        "        # Use ast.literal_eval to convert text with single quotes into a dictionary\n",
        "        if line.strip(): # Only execute if the line is not empty\n",
        "            data_list.append(ast.literal_eval(line.strip()))\n",
        "\n",
        "# Convert the collected list of dictionaries into a DataFrame.\n",
        "df_meta_books = pd.DataFrame(data_list)\n",
        "print(df_meta_books.head())\n",
        "print(df_meta_books.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxASi34T_UoY"
      },
      "source": [
        "# Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICWKmhnW_dlG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import os\n",
        "\n",
        "# --- Helper Function ---\n",
        "def extract_first_author_id(authors_list):\n",
        "    \"\"\"\n",
        "    Function to extract 'author_id' from Goodreads 'authors' column\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if isinstance(authors_list, list) and len(authors_list) > 0:\n",
        "            first_author = authors_list[0]\n",
        "            if isinstance(first_author, dict):\n",
        "                return first_author.get('author_id')\n",
        "    except Exception as e:\n",
        "        pass\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JABU6GvO_idy"
      },
      "outputs": [],
      "source": [
        "# --- Define File Paths ---\n",
        "BASE_PATH = '/content/drive/MyDrive/data/'\n",
        "path_d1_books   = os.path.join(BASE_PATH, 'goodreads_books.json')\n",
        "path_d1_authors = os.path.join(BASE_PATH, 'goodreads_book_authors.json')\n",
        "path_d1_reviews = os.path.join(BASE_PATH, 'goodreads_reviews_dedup.json')\n",
        "path_d2_meta    = os.path.join(BASE_PATH, 'meta_Books.json')\n",
        "path_d2_reviews = os.path.join(BASE_PATH, 'Books_5.json')\n",
        "\n",
        "FINAL_GOODREADS_CSV = os.path.join(BASE_PATH, 'df_goodreads_final.csv')\n",
        "FINAL_AMAZON_CSV    = os.path.join(BASE_PATH, 'df_amazon_final.csv')\n",
        "\n",
        "# --- Settings ---\n",
        "TARGET_MASTER_COUNT = 70_000 # Number of common books. Limited to 70k due to large data size\n",
        "CHUNK_SIZE = 100_000 # Amount of data to load into RAM at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqWmtlYa_lWg"
      },
      "outputs": [],
      "source": [
        "# --- 1. Load Author Mapping Table ---\n",
        "print(\"\\n--- Loading Goodreads Author (D1 Authors) Mapping Table ---\")\n",
        "df_authors_full = pd.read_json(path_d1_authors, lines=True, encoding='latin-1')\n",
        "df_authors_full['Book-Author-ID'] = df_authors_full['author_id'].astype(str)\n",
        "author_id_to_name_map = df_authors_full.set_index('Book-Author-ID')['name'].to_dict() # Convert to a dictionary of {'author_id': 'name'} format\n",
        "print(f\"  > Author mapping table for {len(author_id_to_name_map)} authors created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KP0oaGiH_ss1"
      },
      "outputs": [],
      "source": [
        "# --- 2. Scan Amazon Meta ISBN and Create Set ---\n",
        "# Scan the entire D2(Amazon) metadata file to collect all unique book ISBNs ('asin') present in Amazon.\n",
        "print(\"\\n--- Scanning Amazon Meta (D2 Meta) ISBN 'Set' ---\")\n",
        "amazon_isbn_set = set()\n",
        "with open(path_d2_meta, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if line.strip():\n",
        "            try:\n",
        "                book_dict = ast.literal_eval(line.strip())\n",
        "                if 'asin' in book_dict and book_dict['asin']:\n",
        "                    amazon_isbn_set.add(book_dict['asin']) # Add asin (ISBN) to the set\n",
        "            except:\n",
        "                continue # Skip lines with parsing errors\n",
        "print(f\"  > Collected {len(amazon_isbn_set)} unique Amazon(D2) ISBNs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYFyJVc5_y1U"
      },
      "outputs": [],
      "source": [
        "# --- 3. Scan Goodreads Books and Extract Common Book Candidates ---\n",
        "# Scan the D1(Goodreads) book list (tens of millions) and find 70,000 common books whose ISBN exists in the 'amazon_isbn_set' created in step 2.\n",
        "print(f\"\\n--- Step 3: Scanning D1(Goodreads Books) and extracting {TARGET_MASTER_COUNT} common book candidates based on Amazon Set ---\")\n",
        "master_book_candidates = [] # List to store temporary candidates\n",
        "found_count = 0             # Counter for the number found so far\n",
        "\n",
        "# Read in chunks\n",
        "d1_books_iterator = pd.read_json(\n",
        "    path_d1_books,\n",
        "    lines=True,\n",
        "    encoding='latin-1',\n",
        "    chunksize=CHUNK_SIZE\n",
        ")\n",
        "\n",
        "for chunk in d1_books_iterator:\n",
        "    chunk_filtered = chunk[chunk['isbn'].isin(amazon_isbn_set)].copy()\n",
        "\n",
        "    # If common books are found, slice the required number so as not to exceed the count\n",
        "    if not chunk_filtered.empty:\n",
        "        needed = TARGET_MASTER_COUNT - found_count\n",
        "        if len(chunk_filtered) > needed:\n",
        "            chunk_filtered = chunk_filtered.iloc[:needed]\n",
        "\n",
        "        chunk_filtered['Book-Author-ID'] = chunk_filtered['authors'].apply(extract_first_author_id)\n",
        "\n",
        "        # Select only the final columns to be used\n",
        "        d1_master_cols = ['book_id', 'isbn', 'title', 'publisher', 'Book-Author-ID']\n",
        "        d1_cols_final = [col for col in d1_master_cols if col in chunk_filtered.columns]\n",
        "        chunk_ready = chunk_filtered[d1_cols_final].copy()\n",
        "\n",
        "        # Add to the master_book_candidates list\n",
        "        master_book_candidates.append(chunk_ready)\n",
        "        found_count += len(chunk_ready)\n",
        "        print(f\"  ... {found_count} / {TARGET_MASTER_COUNT} found\") # Print progress\n",
        "\n",
        "    if found_count >= TARGET_MASTER_COUNT:\n",
        "        print(f\"  > Target of {TARGET_MASTER_COUNT} reached. Stopping scan.\") # Print progress\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QeKIPn8_3T6"
      },
      "outputs": [],
      "source": [
        "# --- 4. Complete df_master_books ---\n",
        "print(\"\\n--- Step 4: Completing df_master_books (Common Books) ---\")\n",
        "if master_book_candidates:\n",
        "    df_master_books = pd.concat(master_book_candidates, ignore_index=True)\n",
        "    df_master_books = df_master_books.head(TARGET_MASTER_COUNT)\n",
        "\n",
        "    # Change author id to author name\n",
        "    df_master_books['Book-Author'] = df_master_books['Book-Author-ID'].map(author_id_to_name_map)\n",
        "    # Standardize column names\n",
        "    df_master_books = df_master_books.rename(columns={\n",
        "        'isbn': 'ISBN', 'title': 'Book-Title', 'publisher': 'Publisher'\n",
        "    })\n",
        "    final_master_cols = ['book_id', 'ISBN', 'Book-Title', 'Publisher', 'Book-Author']\n",
        "    df_master_books = df_master_books[final_master_cols]\n",
        "    print(f\"  > Final {len(df_master_books)} master books confirmed. (Loaded into memory)\") # [Print progress]\n",
        "else:\n",
        "    print(\"  > Warning: No common books were found.\") # [Print progress]\n",
        "    df_master_books = pd.DataFrame(columns=['book_id', 'ISBN', 'Book-Title', 'Publisher', 'Book-Author'])\n",
        "\n",
        "# Define sets for filtering\n",
        "master_book_id_set_final = set(df_master_books['book_id'])\n",
        "master_isbn_set_final = set(df_master_books['ISBN'])\n",
        "# Define final csv column order\n",
        "final_columns = ['User-ID', 'ISBN', 'Book-Rating', 'Book-Title', 'Book-Author', 'Publisher', 'Review']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hvs6kRnz_7su"
      },
      "outputs": [],
      "source": [
        "# --- 5. Create Final Goodreads DataFrame (Write directly to CSV) ---\n",
        "# Scan all D1(Goodreads) reviews, select only those corresponding to df_master_books, and save\n",
        "print(f\"\\n--- Step 5: Scanning D1(Goodreads Reviews) and saving to CSV -> {FINAL_GOODREADS_CSV} ---\")\n",
        "is_first_chunk = True\n",
        "if master_book_id_set_final:\n",
        "    d1_reviews_iterator = pd.read_json(\n",
        "        path_d1_reviews, lines=True, encoding='latin-1', chunksize=CHUNK_SIZE\n",
        "    )\n",
        "    for i, chunk in enumerate(d1_reviews_iterator):\n",
        "        print(f\"   > Processing D1 Reviews chunk {i+1}...\") # [Print progress]\n",
        "        chunk_filtered = chunk[chunk['book_id'].isin(master_book_id_set_final)].copy()\n",
        "\n",
        "        if not chunk_filtered.empty:\n",
        "            # Standardize column names\n",
        "            d1_rating_cols = ['user_id', 'book_id', 'rating', 'review_text']\n",
        "            chunk_ready = chunk_filtered[d1_rating_cols].copy()\n",
        "            chunk_ready = chunk_ready.rename(columns={\n",
        "                'user_id': 'User-ID', 'rating': 'Book-Rating', 'review_text': 'Review'\n",
        "            })\n",
        "\n",
        "            # Merge reviews and master_books based on book_id\n",
        "            df_goodreads_chunk_final = pd.merge(\n",
        "                chunk_ready, df_master_books, on='book_id', how='inner'\n",
        "            )\n",
        "            df_goodreads_chunk_final = df_goodreads_chunk_final[final_columns]\n",
        "\n",
        "            # First chunk: write mode creates a new file and includes header\n",
        "            if is_first_chunk:\n",
        "                df_goodreads_chunk_final.to_csv(FINAL_GOODREADS_CSV, mode='w', header=True, index=False, encoding='utf-8')\n",
        "                is_first_chunk = False\n",
        "\n",
        "            # Second chunk: append mode adds data only\n",
        "            else:\n",
        "                df_goodreads_chunk_final.to_csv(FINAL_GOODREADS_CSV, mode='a', header=False, index=False, encoding='utf-8')\n",
        "else:\n",
        "    print(\"   > Master book list (book_id) is empty. Skipping Goodreads reviews scan.\") # [Print progress]\n",
        "\n",
        "print(f\"   > [df_goodreads_final] save complete: {FINAL_GOODREADS_CSV}\") # [Print progress]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kuye0vfN__-4"
      },
      "outputs": [],
      "source": [
        "# --- 6. Create Final Amazon DataFrame (Write directly to CSV) ---\n",
        "# Scan all D2(Amazon Reviews) reviews, select only those corresponding to df_master_books, and save\n",
        "print(f\"\\n--- Step 6: Scanning D2(Amazon Reviews) and saving to CSV -> {FINAL_AMAZON_CSV} ---\")\n",
        "is_first_chunk = True\n",
        "if master_isbn_set_final:\n",
        "    d2_reviews_iterator = pd.read_json(\n",
        "        path_d2_reviews, lines=True, encoding='latin-1', chunksize=CHUNK_SIZE\n",
        "    )\n",
        "    for i, chunk in enumerate(d2_reviews_iterator):\n",
        "        print(f\"   > Processing D2 Reviews chunk {i+1}...\") # [Print progress]\n",
        "        chunk_filtered = chunk[chunk['asin'].isin(master_isbn_set_final)].copy()\n",
        "\n",
        "        if not chunk_filtered.empty:\n",
        "            # Standardize column names\n",
        "            d2_rating_cols = ['reviewerID', 'asin', 'overall', 'reviewText']\n",
        "            chunk_ready = chunk_filtered[d2_rating_cols].copy()\n",
        "            chunk_ready = chunk_ready.rename(columns={\n",
        "                'asin': 'ISBN', 'reviewerID': 'User-ID', 'overall': 'Book-Rating', 'reviewText': 'Review'\n",
        "            })\n",
        "\n",
        "            # Merge reviews and master_books based on ISBN\n",
        "            df_amazon_chunk_final = pd.merge(\n",
        "                chunk_ready,\n",
        "                df_master_books[['ISBN', 'Book-Title', 'Book-Author', 'Publisher']],\n",
        "                on='ISBN',\n",
        "                how='inner'\n",
        "            )\n",
        "            df_amazon_chunk_final = df_amazon_chunk_final[final_columns]\n",
        "\n",
        "            # First chunk: write mode creates a new file and includes header\n",
        "            if is_first_chunk:\n",
        "                df_amazon_chunk_final.to_csv(FINAL_AMAZON_CSV, mode='w', header=True, index=False, encoding='utf-8')\n",
        "                is_first_chunk = False\n",
        "\n",
        "            # Second chunk: append mode adds data only\n",
        "            else:\n",
        "                df_amazon_chunk_final.to_csv(FINAL_AMAZON_CSV, mode='a', header=False, index=False, encoding='utf-8')\n",
        "else:\n",
        "    print(\"   > Master book list (ISBN) is empty. Skipping Amazon reviews scan.\") # [Print progress]\n",
        "\n",
        "print(f\"   > [df_amazon_final] save complete: {FINAL_AMAZON_CSV}\") # [Print progress]\n",
        "\n",
        "print(\"\\n--- All steps complete ---\") # [Print progress]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eldjbHn7Uqhy"
      },
      "outputs": [],
      "source": [
        "# Check dataset (amazon, goodreads) info\n",
        "\n",
        "df_amazon_final = pd.read_csv(\n",
        "    '/content/drive/MyDrive/data/df_amazon_final.csv',\n",
        "    encoding='latin-1'\n",
        ")\n",
        "\n",
        "print(df_amazon_final.head())\n",
        "print(df_amazon_final.info())\n",
        "\n",
        "df_goodreads_book_authors = pd.read_csv(\n",
        "    '/content/drive/MyDrive/data/df_goodreads_final.csv',\n",
        "    encoding='latin-1'\n",
        ")\n",
        "\n",
        "print(df_goodreads_book_authors.head())\n",
        "print(df_goodreads_book_authors.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiJny1UZ-zwS"
      },
      "source": [
        "# Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CQd1wL8zBvN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "\n",
        "df_amazon_final = pd.read_csv(\n",
        "    '/content/drive/MyDrive/data/df_amazon_final.csv',\n",
        "    encoding='latin-1'\n",
        ")\n",
        "print(df_amazon_final.info())\n",
        "\n",
        "df_goodreads_final = pd.read_csv(\n",
        "    '/content/drive/MyDrive/data/df_goodreads_final.csv',\n",
        "    encoding='latin-1'\n",
        ")\n",
        "print(df_goodreads_final.info())\n",
        "\n",
        "\n",
        "# Remove missing values\n",
        "# Allow empty Publisher\n",
        "# Drop rows where the 'Review' column has NaN\n",
        "df_amazon_final = df_amazon_final.dropna(subset=['Review'])\n",
        "df_goodreads_final = df_goodreads_final.dropna(subset=['Review'])\n",
        "\n",
        "print(\"Amazon (After removal):\")\n",
        "print(df_amazon_final.info())\n",
        "\n",
        "print(\"\\nGoodreads (After removal):\")\n",
        "print(df_goodreads_final.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUwDBTfnJHuu"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqyLtyVNJOku"
      },
      "outputs": [],
      "source": [
        "# D1(goodreads)\n",
        "total_ratings_d1 = len(df_goodreads_final)\n",
        "unique_users_d1 = df_goodreads_final['User-ID'].nunique()\n",
        "unique_books_d1 = df_goodreads_final['ISBN'].nunique()\n",
        "\n",
        "# D2(amazon)\n",
        "total_ratings_d2 = len(df_amazon_final)\n",
        "unique_users_d2 = df_amazon_final['User-ID'].nunique()\n",
        "unique_books_d2 = df_amazon_final['ISBN'].nunique()\n",
        "\n",
        "print(\"[Goodreads]\")\n",
        "print(f\"  Total ratings: {total_ratings_d1}\")\n",
        "print(f\"  Unique users: {unique_users_d1}\")\n",
        "print(f\"  Unique books: {unique_books_d1}\")\n",
        "print(f\"  -> Avg ratings per user: {total_ratings_d1 / unique_users_d1:.2f}\")\n",
        "print(f\"  -> Avg ratings per book: {total_ratings_d1 / unique_books_d1:.2f}\")\n",
        "\n",
        "\n",
        "print(\"\\n[Amazon Product]\")\n",
        "print(f\"  Total ratings: {total_ratings_d2}\")\n",
        "print(f\"  Unique users: {unique_users_d2}\")\n",
        "print(f\"  Unique books: {unique_books_d2}\")\n",
        "print(f\"  -> Avg ratings per user: {total_ratings_d2 / unique_users_d2:.2f}\")\n",
        "print(f\"  -> Avg ratings per book: {total_ratings_d2 / unique_books_d2:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aeaHs-QH5Hy9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import warnings\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Ignore warning messages\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Long-Tail Distribution\n",
        "\n",
        "print(\"\\n--- Long-Tail Distribution ---\")\n",
        "\n",
        "gr_user_counts = df_goodreads_final['User-ID'].value_counts()\n",
        "gr_item_counts = df_goodreads_final['ISBN'].value_counts()\n",
        "amz_user_counts = df_amazon_final['User-ID'].value_counts()\n",
        "amz_item_counts = df_amazon_final['ISBN'].value_counts()\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Long-Tail Distribution Visualization (Log Scale)', fontsize=16)\n",
        "\n",
        "# (D1) Goodreads Users\n",
        "sns.histplot(gr_user_counts, ax=axes[0, 0], bins=50, log_scale=True)\n",
        "axes[0, 0].set_title('[D1] Goodreads User Rating Counts (Log Scale)')\n",
        "axes[0, 0].set_xlabel('Number of Ratings')\n",
        "\n",
        "# (D1) Goodreads Books\n",
        "sns.histplot(gr_item_counts, ax=axes[0, 1], bins=50, log_scale=True)\n",
        "axes[0, 1].set_title('[D1] Goodreads Book Rating Counts (Log Scale)')\n",
        "axes[0, 1].set_xlabel('Number of Ratings')\n",
        "\n",
        "# (D2) Amazon Users\n",
        "sns.histplot(amz_user_counts, ax=axes[1, 0], bins=50, log_scale=True)\n",
        "axes[1, 0].set_title('[D2] Amazon User Rating Counts (Log Scale)')\n",
        "axes[1, 0].set_xlabel('Number of Ratings')\n",
        "\n",
        "# (D2) Amazon Books\n",
        "sns.histplot(amz_item_counts, ax=axes[1, 1], bins=50, log_scale=True)\n",
        "axes[1, 1].set_title('[D2] Amazon Book Rating Counts (Log Scale)')\n",
        "axes[1, 1].set_xlabel('Number of Ratings')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Popularity Bias\n",
        "\n",
        "print(\"\\n--- Popularity Bias ---\")\n",
        "gr_item_stats = df_goodreads_final[df_goodreads_final['Book-Rating'] > 0].groupby('ISBN')['Book-Rating'].agg(['count', 'mean']).reset_index()\n",
        "gr_item_stats.columns = ['ISBN', 'rating_count', 'mean_rating']\n",
        "\n",
        "# Amazon (D2)\n",
        "amz_item_stats = df_amazon_final.groupby('ISBN')['Book-Rating'].agg(['count', 'mean']).reset_index()\n",
        "amz_item_stats.columns = ['ISBN', 'rating_count', 'mean_rating']\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "fig.suptitle('Popularity Bias', fontsize=16)\n",
        "\n",
        "# (D1) Goodreads\n",
        "sns.scatterplot(x='rating_count', y='mean_rating', data=gr_item_stats, ax=axes[0], alpha=0.1, s=10)\n",
        "axes[0].set_title('[D1] Goodreads')\n",
        "axes[0].set_xlabel('Rating Count')\n",
        "axes[0].set_ylabel('Mean Rating')\n",
        "axes[0].set_ylim(0.8, 5.2)\n",
        "axes[0].set_xlim(None, 1000)\n",
        "\n",
        "# (D2) Amazon\n",
        "sns.scatterplot(x='rating_count', y='mean_rating', data=amz_item_stats, ax=axes[1], alpha=0.1, s=10)\n",
        "axes[1].set_title('[D2] Amazon')\n",
        "axes[1].set_xlabel('Rating Count')\n",
        "axes[1].set_ylabel('Mean Rating')\n",
        "axes[1].set_ylim(0.8, 5.2)\n",
        "axes[1].set_xlim(None, 500)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- Sentiment Comparison Top Keywords (2-Gram) ---\")\n",
        "\n",
        "# Sample data for efficiency\n",
        "SAMPLE_SIZE = 50000\n",
        "gr_sample = df_goodreads_final.sample(n=SAMPLE_SIZE, replace=True, random_state=42)\n",
        "amz_sample = df_amazon_final.sample(n=SAMPLE_SIZE, replace=True, random_state=42)\n",
        "\n",
        "# Exclude 0-ratings\n",
        "gr_sample = gr_sample[gr_sample['Book-Rating'] > 0]\n",
        "amz_sample = amz_sample[amz_sample['Book-Rating'] > 0]\n",
        "\n",
        "\n",
        "# Prepare text data (Positive vs. Negative)\n",
        "# D1: 4, 5 = Positive / 1, 2 = Negative\n",
        "gr_pos_text_list = gr_sample[gr_sample['Book-Rating'].isin([4, 5])]['Review'].dropna().tolist()\n",
        "gr_neg_text_list = gr_sample[gr_sample['Book-Rating'].isin([1, 2])]['Review'].dropna().tolist()\n",
        "\n",
        "# D2: 4, 5 = Positive / 1, 2 = Negative\n",
        "amz_pos_text_list = amz_sample[amz_sample['Book-Rating'].isin([4, 5])]['Review'].dropna().tolist()\n",
        "amz_neg_text_list = amz_sample[amz_sample['Book-Rating'].isin([1, 2])]['Review'].dropna().tolist()\n",
        "\n",
        "# Stopwords\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.update([\n",
        "    \"book\", \"read\", \"books\", \"reading\", \"one\", \"like\", \"even\", \"would\",\n",
        "    \"really\", \"story\", \"get\", \"much\", \"Amazon\", \"Goodreads\", \"review\",\n",
        "    \"author\", \"title\", \"publisher\", \"ISBN\"\n",
        "])\n",
        "\n",
        "# Helper function: Get Top-N Bigrams\n",
        "def get_top_n_bigrams(corpus, n=20, custom_stopwords=None):\n",
        "    if not corpus: # If corpus is empty\n",
        "        return pd.DataFrame(columns=['bigram', 'count'])\n",
        "\n",
        "    try:\n",
        "        # CountVectorizer: Get bigram frequencies\n",
        "        vec = CountVectorizer(stop_words=list(custom_stopwords),\n",
        "                                max_features=n, # Top N features\n",
        "                                ngram_range=(2, 2) # Bigrams only\n",
        "                               ).fit(corpus)\n",
        "\n",
        "        # Create bag-of-words\n",
        "        bag_of_words = vec.transform(corpus)\n",
        "        # Sum frequencies\n",
        "        sum_words = bag_of_words.sum(axis=0)\n",
        "        # Map words to frequencies\n",
        "        bigrams_freq = [(bigram, sum_words[0, idx]) for bigram, idx in vec.vocabulary_.items()]\n",
        "        # Sort by frequency\n",
        "        bigrams_freq = sorted(bigrams_freq, key = lambda x: x[1], reverse=True)\n",
        "\n",
        "        return pd.DataFrame(bigrams_freq, columns=['bigram', 'count'])\n",
        "\n",
        "    except ValueError: # e.g., corpus is all stopwords\n",
        "        return pd.DataFrame(columns=['bigram', 'count'])\n",
        "\n",
        "# Calculate and print results\n",
        "print(\"\\n--- [D1] Goodreads Top 20 Bigrams ---\")\n",
        "print(\"\\n[D1] Positive (4-5 Stars):\")\n",
        "# Call the helper function\n",
        "print(get_top_n_bigrams(gr_pos_text_list, n=20, custom_stopwords=stopwords))\n",
        "print(\"\\n[D1] Negative (1-2 Stars):\")\n",
        "print(get_top_n_bigrams(gr_neg_text_list, n=20, custom_stopwords=stopwords))\n",
        "\n",
        "print(\"\\n--- [D2] Amazon Top 20 Bigrams ---\")\n",
        "print(\"\\n[D2] Positive (4-5 Stars):\")\n",
        "print(get_top_n_bigrams(amz_pos_text_list, n=20, custom_stopwords=stopwords))\n",
        "print(\"\\n[D2] Negative (1-2 Stars):\")\n",
        "print(get_top_n_bigrams(amz_neg_text_list, n=20, custom_stopwords=stopwords))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMxkaP1led9d"
      },
      "outputs": [],
      "source": [
        "df_goodreads_final.to_csv(FINAL_GOODREADS_CSV, index=False, encoding='utf-8')\n",
        "df_amazon_final.to_csv(FINAL_AMAZON_CSV, index=False, encoding='utf-8')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}